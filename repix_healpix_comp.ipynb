{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tag_id = os.environ.get(\"tag_id\", \"SV_A11981\")\n",
    "model = os.environ.get(\"model\", \"merged\")\n",
    "healpy = os.environ.get(\"healpy\", \"4\")\n",
    "\n",
    "basepath = os.environ.get(\"basepath\", \"../data_local/\")\n",
    "basepath = os.environ.get(\"basepath\", \"/home/datawork-lops-iaocea/work/fish/marc/\")\n",
    "\n",
    "input_filename = basepath + \"diff/\" + tag_id + \"-\" + model + \".zarr\"\n",
    "\n",
    "nside = 4096 * 2 * 2\n",
    "if healpy == \"1\":\n",
    "    nside = 4096\n",
    "param_filename = \"../data_local/healpixbase_nside_\" + str(nside) + \"_\" + model + \".nc\"\n",
    "\n",
    "output_filename = (\n",
    "    basepath + \"diff_healpix/\" + tag_id + \"-\" + model + \"-\" + str(nside) + \".zarr\"\n",
    ")\n",
    "\n",
    "\n",
    "tag_id, model, healpy, basepath, input_filename, nside, output_filename, param_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(input_filename, engine=\"zarr\", chunks={})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "param = xr.open_dataset(param_filename)\n",
    "nside2 = param.nside2.item()\n",
    "p2 = param.p2.load()  # .data\n",
    "w = param.w.load()  # .data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parallelised using dask to compute each time step \n",
    "\n",
    "(later, i need to update to optimise the size of cluster with memory.\n",
    "should change the memory requirements depending on the sizer of nside) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_hpcconfig\n",
    "\n",
    "# cluster = dask_hpcconfig.cluster(\"datarmor-local\")\n",
    "from distributed import Client\n",
    "\n",
    "overrides = {\"cluster.processes\": 7}\n",
    "cluster = dask_hpcconfig.cluster(\"datarmor\", **overrides)\n",
    "cluster.scale(56)\n",
    "\n",
    "# cluster = dask_hpcconfig.cluster(\"datarmor-local\")\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#             from dask.distributed import Client, LocalCluster\n",
    "#             cluster = PBSCluster()\n",
    "#             cluster.scale(5*28)\n",
    "#             client = Client(cluster)\n",
    "#             client.wait_for_workers(4*28)\n",
    "import dask_jobqueue\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "extra_args=[\n",
    "        \"-m n\"\n",
    "           ]\n",
    "\n",
    "cluster = PBSCluster(processes=7,\n",
    "                       cores=28,\n",
    "                       #threads=4,\n",
    "                       memory=\"120GB\",\n",
    "                       #project=\"gen7420@rome\",\n",
    "                       walltime=\"24:00:00\",\n",
    "                       queue=\"mpi_1\",\n",
    "                       local_directory=\"$TMPDIR\",\n",
    "                       death_timeout=\"250s\",\n",
    "                       name=\"jupy_wow\",\n",
    "                       resource_spec= \"select=1:ncpus=28:mem=120GB\",\n",
    "                       nanny=True,\n",
    "                       interface=\"ib0\",\n",
    "                       job_extra_directives=extra_args\n",
    "                      )\n",
    "#             cluster.scale(5*28)\n",
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cluster.scale(28)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()\n",
    "# client.cluster.adapt(minimum=6, maximum=6)\n",
    "client"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "client.wait_for_workers(n_workers=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regrid(data, pix, weights, nside):\n",
    "    if nside == -1:\n",
    "        nside = nside\n",
    "    b = np.zeros([nside * nside])\n",
    "    # b[:]=np.nan\n",
    "    bh = np.zeros([nside * nside])\n",
    "    # bh[:]=np.nan\n",
    "    for iii in range(4):\n",
    "        b = b + np.bincount(\n",
    "            pix[iii, :],\n",
    "            weights=weights[iii, :] * (data).flatten(),\n",
    "            minlength=nside * nside,\n",
    "        )\n",
    "        bh = bh + np.bincount(\n",
    "            pix[iii, :], weights=weights[iii, :], minlength=nside * nside\n",
    "        )\n",
    "    b[bh > 0] /= bh[bh > 0]\n",
    "    b[bh == 0] = np.nan\n",
    "    del bh\n",
    "    res = b.reshape(nside, nside)\n",
    "    del b\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for not using dask, here you do\n",
    "# ds=ds.compute()\n",
    "#\n",
    "data = xr.apply_ufunc(\n",
    "    regrid,\n",
    "    ds.diff_,\n",
    "    p2,\n",
    "    w,\n",
    "    nside2,\n",
    "    input_core_dims=[[\"nj\", \"ni\"], [\"a\", \"b\"], [\"a\", \"b\"], []],\n",
    "    output_core_dims=[[\"x\", \"y\"]],\n",
    "    exclude_dims=set((\"nj\", \"ni\")),\n",
    "    vectorize=True,\n",
    "    dask=\"parallelized\",\n",
    "    dask_gufunc_kwargs={\n",
    "        \"output_sizes\": {\"x\": nside2, \"y\": nside2},\n",
    "    },\n",
    "    output_dtypes=[ds.diff_.dtype],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_healpy = data.to_dataset(name=\"diff_\")\n",
    "ds_healpy = ds_healpy.assign({\"H0\": param.H0})\n",
    "ds_healpy = (\n",
    "    ds_healpy.assign_coords({\"longitude\": param.longitude})\n",
    "    .assign_coords({\"latitude\": param.latitude})\n",
    "    .assign_coords({\"x\": param.x})\n",
    "    .assign_coords({\"y\": param.y})\n",
    "    .assign_attrs({\"tag_id\": tag_id})\n",
    "    .assign_attrs({\"grid_size\": param.attrs[\"grid_size\"]})\n",
    "    .chunk({\"time\": 1, \"x\": -1, \"y\": -1})\n",
    ")\n",
    "\n",
    "ds_healpy = ds_healpy.dropna(dim=\"x\", how=\"all\", subset=[\"H0\"]).dropna(\n",
    "    dim=\"y\", how=\"all\", subset=[\"H0\"]\n",
    ")\n",
    "\n",
    "\n",
    "ds_healpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_healpy.to_zarr(output_filename, mode=\"w\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "ds_healpy=ds_healpy.persist()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import hvplot\n",
    "import hvplot.xarray\n",
    "(ds_healpy.diff_.hvplot.quadmesh(\n",
    "            x='longitude'\n",
    "            ,y='latitude'\n",
    "            ,clabel='merged'\n",
    "            ,rasterize=True\n",
    "            , geo=True\n",
    "            ,coastline=\"110m\"\n",
    "            ,clim=(0,10)\n",
    "            ) + \n",
    " ds_healpy.H0.hvplot.quadmesh(\n",
    "            x='longitude'\n",
    "            ,y='latitude'\n",
    "            ,clabel='merged'\n",
    "            ,rasterize=True\n",
    "            , geo=True\n",
    "            ,coastline=\"110m\"\n",
    "            ,clim=(0,10)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
