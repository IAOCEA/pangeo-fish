{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e6f639-2455-4f5d-a557-b78b1d821ecf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# **Example Usage of Pangeo-Fish Software**\n",
    "\n",
    "**Overview:**\n",
    "This Jupyter notebook demonstrates the usage of the pangeo-fish software, a tool designed for analyzing biologging data in reference to Earth Observation (EO) data. Specifically, it utilizes data from the biologging tag 'A19124' and reference data from the European Union Copernicus Marine Service Information (CMEMS) product 'NORTHWESTSHELF_ANALYSIS_FORECAST_PHY_004_013,' which were employed in the study conducted by M. Gonze et al. titled \"Combining acoustic telemetry with archival tagging to investigate the spatial dynamics of the understudied pollack *Pollachius pollachius*,\" published in the Journal of Fish Biology.\n",
    "\n",
    "**Purpose:**\n",
    "By executing this notebook, users will learn how to set up a workflow for utilizing the pangeo-fish software. The workflow involves ten steps outlined below:\n",
    "\n",
    "1. **Configure the Notebook:** Prepare the notebook environment for analysis.\n",
    "2. **Compare Reference Model with DST Tag Information:** Analyze and compare data from the reference model with information from the biologging data.\n",
    "3. **Regrid the Grid from Reference Model Grid to Healpix Grid:** Transform the grid from the reference model grid to the healpix grid for further analysis.\n",
    "4. **Construct Emission Matrix:** Create an emission matrix based on the transformed grid.\n",
    "5. **Compute Additional Emission Probability Matrix:** Calculate additional emission probability matrix, particularly focusing on teledetection from acoustic signals.\n",
    "6. **Combine and Normalize Emission Matrix:** Merge the emission matrix and normalize it for further processing.\n",
    "7. **Estimate Model Parameters:** Determine the parameters of the model based on the normalized emission matrix.\n",
    "8. **Compute State Probabilities:** Calculate the probabilities associated with different states within the model.\n",
    "9. **Compute Tracks:** Analyze and compute tracks based on the model parameters and state probabilities.\n",
    "10. **Visualization:** Visualize the results of the analysis for interpretation and insight.\n",
    "\n",
    "Throughout this notebook, users will gain practical experience in setting up and executing a workflow using pangeo-fish, enabling them to apply similar methodologies to their own biologging data analysis tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c535925e-793d-41be-a989-4fae4cdaaa67",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. **Configure the Notebook:** Prepare the notebook environment for analysis.\n",
    "This section sets up the notebook environment for analysis. It includes installing necessary packages, importing required libraries, setting up parameters, and configuring the cluster for distributed computing. It also retrieves the tag data needed for analysis.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5cbc9aa-2487-4542-8187-08954751b516",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Run the following 3 lines of command to install pangeo-fish in the pangeo environment.\n",
    "# Note: These commands install required packages for the analysis.  \n",
    "# You may need to restart your kernel before executing the next cell.\n",
    "#\n",
    "\n",
    "!pip install git+https://github.com/iaocea/xarray-healpy\n",
    "!pip install rich dask_image zstandard xmovie\n",
    "!pip install -e ../."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea188b-a3b6-4d00-9cf4-7c99c811f809",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules for data analysis.\n",
    "import dask\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pint_xarray\n",
    "import xarray as xr\n",
    "from pint_xarray import unit_registry as ureg\n",
    "from pangeo_fish.io import open_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dd510f-d3d2-4474-a983-2ebac4f13b45",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Set up execution parameters for the analysis.\n",
    "# Note: This cell is tagged as parameters, allowing automatic updates when configuring with papermil.\n",
    "\n",
    "# tag_name corresponds to the name of the biologging tag name (DST identification number), \n",
    "# which is also a path for storing all the information for the specific fish tagged with tag_name.\n",
    "tag_name = \"A19124\"  \n",
    "\n",
    "# tag_root specifies the root URL for tag data used for this computation.\n",
    "tag_root = \"https://data-taos.ifremer.fr/data_tmp/cleaned/tag/\"\n",
    "\n",
    "# catalog_url specifies the URL for the catalog for reference data used.\n",
    "catalog_url = \"https://data-taos.ifremer.fr/kerchunk/ref-copernicus.yaml\"\n",
    "\n",
    "# scratch_root specifies the root directory for storing output files.\n",
    "scratch_root = \"s3://destine-gfts-data-lake/demo\"\n",
    "\n",
    "# storage_options specifies options for the filesystem storing output files.\n",
    "storage_options = {\n",
    "    'anon': False, \n",
    "    'profile' : \"gfts\",\n",
    "    'client_kwargs': {\n",
    "        \"endpoint_url\": \"https://s3.gra.perf.cloud.ovh.net\",\n",
    "        \"region_name\": \"gra\",\n",
    "    }\n",
    "}\n",
    "# Default chunk value for time dimension.  This values depends on the configuration of your dask cluster.\n",
    "chunk_time=1\n",
    "\n",
    "\n",
    "\n",
    "# if you are using local file system, activate following two lines\n",
    "# scratch_root = \".\"\n",
    "# storage_options = None\n",
    "\n",
    "#\n",
    "# Parameters for Workflow 2. **Compare Reference Model with DST Tag Information:**\n",
    "#\n",
    "# bbox, bounding box, defines the latitude and longitude range for the analysis area.\n",
    "bbox = {\"lat\": [47, 51], \"lon\": [-8, -1]} \n",
    "\n",
    "# relative_depth_threshold defines the acceptable fish depth relative to the maximum tag depth.\n",
    "# It determines whether the fish can be considered to be in a certain location based on depth.\n",
    "relative_depth_threshold = 0.6\n",
    "\n",
    "#\n",
    "# Parameters for Workflow 3. **Regrid the Grid from Reference Model Grid to Healpix Grid:** \n",
    "#\n",
    "# nside defines the resolution of the healpix grid used for regridding.\n",
    "nside = 4096*2\n",
    "\n",
    "# rot defines the rotation angles for the healpix grid.\n",
    "rot = {\"lat\": 0, \"lon\": 0}\n",
    "\n",
    "# min_vertices sets the minimum number of vertices for a valid transcription for regridding.\n",
    "min_vertices = 1\n",
    "\n",
    "#\n",
    "# Parameters for Workflow 4. **Construct Emission Matrix:**\n",
    "#\n",
    "# differences_std sets the standard deviation for scipy.stats.norm.pdf.\n",
    "# It expresses the estimated certainty of the field of difference.\n",
    "differences_std = 0.75\n",
    "\n",
    "# recapture_std sets the covariance for recapture event.\n",
    "# It shows the certainty of the final recapture area if it is known.\n",
    "recapture_std = 1e-2\n",
    "\n",
    "# earth_radius defines the radius of the Earth used for distance calculations.\n",
    "earth_radius = ureg.Quantity(6371, \"km\")\n",
    "\n",
    "# maximum_speed sets the maximum allowable speed for the tagged fish.\n",
    "maximum_speed = ureg.Quantity(60, \"km / day\")\n",
    "\n",
    "# adjustment_factor adjusts parameters for a more fuzzy search.\n",
    "# It will factor the allowed maximum displacement of the fish.\n",
    "adjustment_factor = 2.5  \n",
    "\n",
    "# truncate sets the truncating factor for computed maximum allowed sigma for convolution process.\n",
    "truncate = 1\n",
    "\n",
    "#\n",
    "# Parameters for Workflow 5. **Compute Additional Emission Probability Matrix:**\n",
    "#\n",
    "# receiver_buffer sets the maximum allowed detection distance for acoustic receivers.\n",
    "receiver_buffer = ureg.Quantity(1000, \"m\")\n",
    "\n",
    "#\n",
    "# Parameters for Workflow 7. **Estimate Model Parameters:** \n",
    "#\n",
    "# tolerance sets the tolerance level for EagerBoundsSearch calculations.\n",
    "tolerance = 1e-2\n",
    "\n",
    "#\n",
    "# Parameters for Workflow 9. **Compute Tracks:**\n",
    "#\n",
    "# track_modes defines the modes for track calculation.\n",
    "track_modes = [\"mean\", \"mode\"]\n",
    "\n",
    "# additional_track_quantities sets quantities to compute for tracks.\n",
    "additional_track_quantities = [\"speed\", \"distance\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a19e76-606c-4a2f-a93f-b5d2c990823f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define target and tracks root directories for storing analysis results.\n",
    "target_root = f\"{scratch_root}/{tag_name}\"\n",
    "tracks_root = f\"{target_root}/tracks\"\n",
    "# Defines default chunk size for optimisation.  \n",
    "default_chunk = {\"time\": chunk_time, \"lat\": -1, \"lon\": -1}\n",
    "default_chunk_xy = {\"time\": chunk_time, \"x\": -1, \"y\": -1}\n",
    "default_chunk_xy_auto = {\"time\": \"auto\", \"x\": -1, \"y\": -1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268b5c0-b1e8-4d12-b6c9-b3b7aa54f99b",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up a local cluster for distributed computing.\n",
    "from distributed import LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206aeb3c-9684-4eac-80e8-e94939529747",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Open and retrieve the tag data required for the analysis\n",
    "tag = open_tag(tag_root, tag_name)\n",
    "tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524fe17c-43b2-498b-a06b-91ddfba27b81",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2. **Compare Reference Model with DST Tag Information:**\n",
    "In this step, we compare the reference model data with DST (Data Storage Tag) information.\n",
    "The process involves reading and cleaning the reference model, aligning time, converting depth units, \n",
    "subtracting tag data from the model, and saving the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f50d96f-aac2-4461-8189-53d831310973",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import intake\n",
    "from pangeo_fish.cf import bounds_to_bins\n",
    "from pangeo_fish.diff import diff_z\n",
    "from pangeo_fish.io import open_copernicus_catalog\n",
    "from pangeo_fish.tags import adapt_model_time, reshape_by_bins, to_time_slice\n",
    "\n",
    "# Drop data outside the reference interval\n",
    "time_slice = to_time_slice(tag[\"tagging_events/time\"])\n",
    "tag_log = tag[\"dst\"].ds.sel(time=time_slice)\n",
    "\n",
    "# Open and clean reference model\n",
    "cat = intake.open_catalog(catalog_url)\n",
    "model = open_copernicus_catalog(cat)\n",
    "\n",
    "# Subset the reference_model by \n",
    "# - align model time with the time of tag_log, also\n",
    "# - drop data for depth later that are unlikely due to the observed pressure from tag_log\n",
    "# - defined latitude and longitude of bbox.  \n",
    "#\n",
    "reference_model = (\n",
    "    model.sel(time=adapt_model_time(time_slice))\n",
    "    .sel(lat=slice(*bbox[\"lat\"]), lon=slice(*bbox[\"lon\"]))\n",
    "    .pipe(\n",
    "        lambda ds: ds.sel(\n",
    "            depth=slice(None, (tag_log[\"pressure\"].max() - ds[\"XE\"].min()).compute())\n",
    "        )\n",
    "    )\n",
    ")\n",
    "reference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba30694c-c1eb-49b2-a818-db2deda9decf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Reshape the tag log, so that it bins to the time step of reference_model\n",
    "reshaped_tag = reshape_by_bins(\n",
    "    tag_log,\n",
    "    dim=\"time\",\n",
    "    bins=(\n",
    "        reference_model.cf.add_bounds([\"time\"], output_dim=\"bounds\")\n",
    "        .pipe(bounds_to_bins, bounds_dim=\"bounds\")\n",
    "        .get(\"time_bins\")\n",
    "    ),\n",
    "    bin_dim=\"bincount\",\n",
    "    other_dim=\"obs\",\n",
    ").chunk({\"time\": chunk_time})\n",
    "\n",
    "# Subtract the time_bined tag_log from the reference_model. \n",
    "# Here, for each time_bin, each observed value are compared with the correspoindng depth of reference_model using diff_z function.  \n",
    "#\n",
    "diff = (\n",
    "    diff_z(reference_model, reshaped_tag, depth_threshold=relative_depth_threshold)\n",
    "    .assign_attrs({\"tag_id\": tag_name})\n",
    "    .assign(\n",
    "        {\n",
    "            \"H0\": reference_model[\"H0\"],\n",
    "            \"ocean_mask\": reference_model[\"H0\"].notnull(),\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Persist the diff data\n",
    "diff = diff.chunk(default_chunk).persist()\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74efae4e-53d2-4852-9b20-c5e3028e0c63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify the data\n",
    "diff[\"diff\"].count([\"lat\", \"lon\"]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c0325b-523c-4d58-8319-151183bb1376",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save snapshot to disk\n",
    "diff.to_zarr(\n",
    "    f\"{target_root}/diff.zarr\", mode=\"w\", storage_options=storage_options\n",
    ")\n",
    "\n",
    "# Cleanup\n",
    "del tag_log, cat, model, reference_model, reshaped_tag, diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517857a8-f73f-4627-b628-32e8ed9f1046",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 3. Regrid the grid from reference model grid to healpix grid.\n",
    "\n",
    "In this step, we regrid the data from the reference model grid to a Healpix grid. This process involves defining the Healpix grid, creating the target grid, computing interpolation weights, performing the regridding, and saving the regridded data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d775f-73e5-40fc-8d3e-a50b342ccb3f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from xarray_healpy import HealpyGridInfo, HealpyRegridder\n",
    "from pangeo_fish.grid import center_longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd261f-a25e-483f-9090-72ed65f621cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Open the diff data and performs cleaning operations to prepare it for regridding.\n",
    "\n",
    "ds = (\n",
    "    xr.open_dataset(f\"{target_root}/diff.zarr\", engine=\"zarr\", chunks={},\n",
    "                    storage_options=storage_options)\n",
    "    .pipe(lambda ds: ds.merge(ds[[\"latitude\", \"longitude\"]].compute()))\n",
    "    .swap_dims({\"lat\": \"yi\", \"lon\": \"xi\"})\n",
    "    .drop_vars([\"lat\", \"lon\"])\n",
    ")\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db0e3e-9c9a-4274-a894-8556666a2438",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define the target Healpix grid information\n",
    "grid = HealpyGridInfo(level=int(np.log2(nside)), rot=rot)\n",
    "target_grid = grid.target_grid(ds).pipe(center_longitude, 0)\n",
    "target_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56410ac-7d35-478c-a201-cca8079032a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute the interpolation weights for regridding the diff data\n",
    "regridder = HealpyRegridder(\n",
    "    ds[[\"longitude\", \"latitude\", \"ocean_mask\"]],\n",
    "    target_grid,\n",
    "    method=\"bilinear\",\n",
    "    interpolation_kwargs={\"mask\": \"ocean_mask\", \"min_vertices\": min_vertices},\n",
    ")\n",
    "regridder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0516c5cd-cfc1-400c-803c-5a5850af3b49",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Perform the regridding operation using the computed interpolation weights.\n",
    "regridded = regridder.regrid_ds(ds)\n",
    "regridded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c1c5f4-9bbc-49b6-a5f5-e26c48563823",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Reshape the regridded data to 2D\n",
    "reshaped = grid.to_2d(regridded).pipe(center_longitude, 0)\n",
    "reshaped = reshaped.persist()\n",
    "reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713698c-1e41-4a61-8db4-2e25fe5e795d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell verifies the regridded data by plotting the count of non-NaN values.\n",
    "reshaped[\"diff\"].count([\"x\", \"y\"]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bc2531-c9cc-4d86-9714-89fcfb345d1e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# This cell saves the regridded data to Zarr format, then cleans up unnecessary variables to free up memory after the regridding process.  \n",
    "reshaped.chunk(default_chunk_xy).to_zarr(\n",
    "    f\"{target_root}/diff-regridded.zarr\",\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    compute=True,\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "# Cleanup unnecessary variables to free up memory\n",
    "del ds, grid, target_grid, regridder, regridded, reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8b02c2-827e-4e0e-bf36-1e1a8187edcc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 4. Construct emission probability matrix\n",
    "\n",
    "In this section, we construct the emission probability matrix based on the differences between the observed tag temperature and the reference sea temperature computed in Workflow 2 and regridded in Workflow 3. The emission probability matrix represents the likelihood of observing a specific temperature difference given the model parameters and configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f68e324-396d-4433-9955-b0ca96f9cd55",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from toolz.dicttoolz import valfilter\n",
    "from pangeo_fish.distributions import create_covariances, normal_at\n",
    "from pangeo_fish.pdf import normal\n",
    "from pangeo_fish.utils import temporal_resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b5abc-e758-4512-adac-9108e8947ee8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Open the regridded diff data \n",
    "differences = xr.open_dataset(\n",
    "    f\"{target_root}/diff-regridded.zarr\",\n",
    "    engine=\"zarr\",\n",
    "    chunks={},\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfdc4e0-1a5b-48ac-82c1-3ce99fc3c8e4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute initial and final position\n",
    "grid = differences[[\"latitude\", \"longitude\"]].compute()\n",
    "\n",
    "initial_position = tag[\"tagging_events\"].ds.sel(event_name=\"release\")\n",
    "cov = create_covariances(1e-6, coord_names=[\"latitude\", \"longitude\"])\n",
    "initial_probability = normal_at(\n",
    "    grid, pos=initial_position, cov=cov, normalize=True, axes=[\"latitude\", \"longitude\"]\n",
    ")\n",
    "\n",
    "final_position = tag[\"tagging_events\"].ds.sel(event_name=\"fish_death\")\n",
    "if final_position[[\"longitude\", \"latitude\"]].to_dataarray().isnull().all():\n",
    "    final_probability = None\n",
    "else:\n",
    "    cov = create_covariances(recapture_std**2, coord_names=[\"latitude\", \"longitude\"])\n",
    "    final_probability = normal_at(\n",
    "        grid,\n",
    "        pos=final_position,\n",
    "        cov=cov,\n",
    "        normalize=True,\n",
    "        axes=[\"latitude\", \"longitude\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccac4fe-cf28-403c-a00b-b78b8cf0e0be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute maximum displacement for each reference model time step\n",
    "# and estimate maximum sigma value for limiting the optimisation step\n",
    "\n",
    "earth_radius_ = xr.DataArray(earth_radius, dims=None)\n",
    "\n",
    "timedelta = temporal_resolution(differences[\"time\"]).pint.quantify().pint.to(\"h\")\n",
    "grid_resolution = earth_radius_ * differences[\"resolution\"].pint.quantify()\n",
    "\n",
    "maximum_speed_ = xr.DataArray(maximum_speed, dims=None).pint.to(\"km / h\")\n",
    "max_grid_displacement = maximum_speed_ * timedelta * adjustment_factor / grid_resolution\n",
    "max_sigma = max_grid_displacement.pint.to(\"dimensionless\").pint.magnitude / truncate\n",
    "max_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04aa3cd-a194-49a3-ae48-9b0a7a3b07ab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "emission_pdf = (\n",
    "    normal(differences[\"diff\"], mean=0, std=differences_std, dims=[\"y\", \"x\"])\n",
    "    .to_dataset(name=\"pdf\")\n",
    "    .assign(\n",
    "        valfilter(\n",
    "            lambda x: x is not None,\n",
    "            {\n",
    "                \"initial\": initial_probability,\n",
    "                \"final\": final_probability,\n",
    "                \"mask\": differences[\"ocean_mask\"],\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "    .assign_attrs(differences.attrs | {\"max_sigma\": max_sigma})\n",
    "    .chunk()\n",
    ")\n",
    "emission_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c066be-cec3-4ab4-96aa-c557f8e00192",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#compute emission probability matrix\n",
    "\n",
    "emission_pdf = (\n",
    "    normal(differences[\"diff\"], mean=0, std=differences_std, dims=[\"y\", \"x\"])\n",
    "    .to_dataset(name=\"pdf\")\n",
    "    .assign(\n",
    "        valfilter(\n",
    "            lambda x: x is not None,\n",
    "            {\n",
    "                \"initial\": initial_probability,\n",
    "                \"final\": final_probability,\n",
    "                \"mask\": differences[\"ocean_mask\"],\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "    .assign_attrs(differences.attrs | {\"max_sigma\": max_sigma})\n",
    ")\n",
    "\n",
    "emission_pdf=emission_pdf.chunk(default_chunk_xy).persist()\n",
    "emission_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe63b9d-dfdc-4370-8def-57a80efcb17c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Verify the data\n",
    "emission_pdf[\"pdf\"].count([\"x\", \"y\"]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a59d9e9-95ad-4611-9eec-776fcb82c329",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell saves the emission data to Zarr format, then cleans up unnecessary variables to free up memory.\n",
    "\n",
    "emission_pdf.to_zarr(\n",
    "    f\"{target_root}/emission.zarr\", mode=\"w\", consolidated=True,\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "\n",
    "\n",
    "del differences, grid, initial_probability, final_probability, emission_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc00b914-1f62-4d50-bbf2-0670368bfd06",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 5. Compute additional emission probability matrix (teledetection from acoustic)\n",
    "In this section, we compute additional emission probabilities based on acoustic detections for the selected tag. These additional probabilities enhance the emission probability matrix constructed in Workflow 4 by incorporating information from acoustic telemetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0d2398-bf16-48df-91a6-1d336da19d77",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# Import necessary libraries and open data and perform initial setup\n",
    "from pangeo_fish import acoustic, utils\n",
    "import hvplot.xarray\n",
    "emission = xr.open_dataset(\n",
    "    f\"{target_root}/emission.zarr\", engine=\"zarr\", chunks={},#\"x\": -1, \"y\": -1},\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcf9542-6b10-4d1b-860c-e6939516566c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct the emission probabilities based on acoustic detections\n",
    "\n",
    "acoustic_pdf = acoustic.emission_probability(\n",
    "    tag, emission[[\"time\", \"cell_ids\", \"mask\"]].compute(), receiver_buffer\n",
    ")\n",
    "acoustic_pdf=acoustic_pdf.persist()\n",
    "acoustic_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493bde96-9b26-4dc7-894d-0f7926dc937c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify the data and visualize the acoustic detections\n",
    "tag['acoustic'][\"deployment_id\"].hvplot.scatter(\n",
    "    c='red',marker='x')*(\n",
    "    acoustic_pdf['acoustic'] != 0).sum(dim=('y', 'x')).hvplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a631b8-0b3f-4519-92d5-3e418be6f5ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge and save the combined emission probability matrix with acoustic probabilities\n",
    "\n",
    "combined = emission.merge(acoustic_pdf)\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f717c-51d4-4169-acb5-be6a6f42c505",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell saves the emission data to Zarr format, then cleans up unnecessary variables to free up memory.\n",
    "\n",
    "combined.to_zarr(\n",
    "    f\"{target_root}/emission_acoustic.zarr\", mode=\"w\", consolidated=True,    \n",
    "    storage_options=storage_options    \n",
    ")\n",
    "#cleanup\n",
    "\n",
    "del emission, acoustic_pdf, combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff35d52d-6caa-4cab-92b7-d796020862a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 6. Combine the emission matrix and normalize\n",
    "In this section, we combine the emission probability matrix constructed in Workflow 4 and 5 then normalize it to ensure that the probabilities sum up to one. This step prepares the combined emission matrix for further analysis and interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ca63d-35e9-4a55-af45-52cea678be17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries \n",
    "from pangeo_fish.pdf import combine_emission_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5c93e-1045-4b57-a148-597a6bfc5727",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open and combine the emission probability matrix\n",
    "\n",
    "combined = (\n",
    "    xr.open_dataset(\n",
    "        f\"{target_root}/emission_acoustic.zarr\",\n",
    "        engine=\"zarr\",\n",
    "        chunks={},\n",
    "        inline_array=True,\n",
    "        storage_options=storage_options          \n",
    "    )\n",
    "    .pipe(combine_emission_pdf)\n",
    "    .chunk(default_chunk_xy_auto)\n",
    "    .persist()  # convert to comment if the emission matrix does *not* fit in memory\n",
    ")\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f6b86e-1d62-402e-a72d-6604a66fa18d",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify the data and visualize the sum of probabilities\n",
    "combined[\"pdf\"].sum([\"x\", \"y\"]).hvplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31505f9-122d-4280-9c29-7cea21cc6e1c",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the combined and normalized emission matrix\n",
    "combined.to_zarr(\n",
    "    f\"{target_root}/combined.zarr\", mode=\"w\", consolidated=True,\n",
    "    storage_options=storage_options    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c408a7c-66ae-4c90-b3dd-1ba46f7ec5ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 7. Estimate the model parameter\n",
    "#    1. select and create estimator instance\n",
    "#    2. create an optimizer using the estimator and the expected parameter range\n",
    "#    3. fit the model to the data to get the model parameter\n",
    "#    4. save\n",
    "\n",
    "import json\n",
    "import fsspec\n",
    "from pangeo_fish.hmm.estimator import EagerScoreEstimator\n",
    "from pangeo_fish.hmm.optimize import EagerBoundsSearch\n",
    "\n",
    "#open the data\n",
    "\n",
    "emission = (\n",
    "    xr.open_dataset(\n",
    "        f\"{target_root}/combined.zarr\",\n",
    "        engine=\"zarr\",\n",
    "        chunks={},\n",
    "        inline_array=True,\n",
    "        storage_options=storage_options            \n",
    "    )\n",
    "    .compute()  # convert to comment if the emission matrix does *not* fit in memory\n",
    ")\n",
    "emission\n",
    "\n",
    "#create and configure estimator and optimizer\n",
    "\n",
    "estimator = EagerScoreEstimator()\n",
    "\n",
    "optimizer = EagerBoundsSearch(\n",
    "    estimator,\n",
    "    (1e-4, emission.attrs[\"max_sigma\"]),\n",
    "    optimizer_kwargs={\"disp\": 3, \"xtol\": tolerance},\n",
    ")\n",
    "\n",
    "#fit the model parameter to the data\n",
    "\n",
    "#%%time\n",
    "optimized = optimizer.fit(emission)\n",
    "optimized\n",
    "\n",
    "#save\n",
    "\n",
    "params = optimized.to_dict()\n",
    "with fsspec.open(#f\"{target_root}/parameters.json\"\n",
    "    \"toto.json\"\n",
    "                 , mode=\"w\",\n",
    "                # storage_options=storage_options   #how do i pass storage_option here?         \n",
    ") as f:\n",
    "    json.dump(params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e41950-c709-4484-8aa1-a69ed4b2cc5d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 8. State probabilities\n",
    "    1. use the configured estimator to predict the state probabilities\n",
    "    2. save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e83b399-ef59-47d8-aa88-8d52a03e4cb9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "recreate the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f29e4e5-9f3a-4ead-8484-8810150e5c4c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with fsspec.open(f\"{target_root}/parameters.json\", mode=\"r\") as f:\n",
    "    params = json.load(f)\n",
    "optimized = EagerScoreEstimator(**params)\n",
    "optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88e4365-79ff-48f6-a2e5-7b7d69a389d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f34555-ea91-47f2-bdb2-73572301082e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "emission = (\n",
    "    xr.open_dataset(\n",
    "        f\"{target_root}/combined.zarr\",\n",
    "        engine=\"zarr\",\n",
    "        chunks=default_chunk_xy_auto,\n",
    "        inline_array=True,\n",
    "        storage_options=storage_options            \n",
    "    )\n",
    ")\n",
    "emission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547dada9-e81a-4d32-aead-e4e6711d216a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "predict the state probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc263e43-a686-482e-91e2-f03bf4483a13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "states = optimized.predict_proba(emission)\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e78f51-60fa-4f43-ac47-cf5be9ba8399",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca9b579-b857-4b93-bfad-70d2f5238dd9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "states.chunk(default_chunk_xy).to_zarr(\n",
    "    f\"{target_root}/states.zarr\", mode=\"w\", consolidated=True,  \n",
    "        storage_options=storage_options                \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4c060d-a410-4895-899b-d8695c15b6db",
   "metadata": {},
   "source": [
    "cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8189b197-1b97-41c1-8e0f-63e9ce36576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25069e5-3b2c-4ba9-a1bc-8ab900221259",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 9. Compute tracks\n",
    "    1. compute mean and mode from the precomputed state probabilities and apply the viterbi algorithm to the emission matrix to get the most probable track\n",
    "    2. save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2973b3-39bf-4ab3-a7b0-5d67f88f52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_fish.hmm.estimator import EagerScoreEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79dc903-0d5b-46be-b06d-1eee5e603ea1",
   "metadata": {},
   "source": [
    "open data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98c5123-d82c-4662-8ca4-7388bcab631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emission = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb46512b-b439-4f7d-a2ff-a291b0cf917d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#put if viterbi here\n",
    "emission = (\n",
    "    xr.open_dataset(\n",
    "        f\"{target_root}/combined.zarr\",\n",
    "        engine=\"zarr\",\n",
    "        chunks=default_chunk_xy_auto,\n",
    "        inline_array=True,\n",
    "        storage_options=storage_options            \n",
    "    ).compute() #\n",
    ")\n",
    "emission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5c58d4-12b2-4e6e-a354-62bec385d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fsspec.open(f\"{target_root}/parameters.json\", mode=\"r\") as f:\n",
    "    params = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b81c874-dc41-4790-b7a3-c3d7a5cda1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized = EagerScoreEstimator(**params)\n",
    "\n",
    "states = xr.open_dataset(\n",
    "    f\"{target_root}/states.zarr\", engine=\"zarr\", chunks={}, inline_array=True,\n",
    "         storage_options=storage_options            \n",
    "   \n",
    ").compute()\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a3997a-775a-49ff-9710-7c447be2219e",
   "metadata": {},
   "source": [
    "decode tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c60f0e5-a29c-48e1-8374-1a33358a24fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trajectories = optimized.decode(\n",
    "    emission,\n",
    "    states,\n",
    "    mode=track_modes,\n",
    "    progress=True,\n",
    "    additional_quantities=additional_track_quantities,\n",
    ")\n",
    "trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a865e8-94a1-4dab-b7c5-1ae1b48c33e1",
   "metadata": {},
   "source": [
    "save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dbfd34-f26f-45dc-b8c7-3c84504e5c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_fish.io import save_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82785f81-e1e1-427c-ac44-168184114704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this need storage option\n",
    "save_trajectories(trajectories, tracks_root, format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7560973-f57f-4ac0-98f9-0fb792dd34e8",
   "metadata": {},
   "source": [
    "cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f925d29-7ad8-4411-8438-eaddf3a2f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "del emission, states, trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530eae69-f7ac-421b-b303-75ec6e4d338f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 10. Visualise results\n",
    "    1. plot the emission matrix\n",
    "    2. plot the state probabilities\n",
    "    3. plot each of the tracks\n",
    "    4. Create movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a71626-1632-488c-987f-fd9dc9f69fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmocean\n",
    "import geopandas as gpd\n",
    "import holoviews as hv\n",
    "import hvplot.xarray\n",
    "import movingpandas as mpd\n",
    "import xmovie\n",
    "\n",
    "from pangeo_fish import visualization\n",
    "from pangeo_fish.io import read_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c717c-eea7-411d-abeb-b11ad9252497",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = read_trajectories(tracks_root, track_modes, format=\"parquet\")\n",
    "trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff656402-ccea-48a9-96dd-bde362b067d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories.hvplot(c=\"speed\", tiles=\"CartoLight\", cmap=\"cmo.speed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0d44b1-0b09-4de0-8864-af6576910c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = [\n",
    "    traj.hvplot(c=\"speed\", tiles=\"CartoLight\", title=traj.id, cmap=\"cmo.speed\")\n",
    "    for traj in trajectories.trajectories\n",
    "]\n",
    "\n",
    "hv.Layout(plots).cols(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d2f27-c728-49f0-9486-be06716136ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "emission = (\n",
    "    xr.open_dataset(\n",
    "        f\"{target_root}/emission-acoustic.zarr\",\n",
    "        engine=\"zarr\",\n",
    "        chunks={},\n",
    "        inline_array=True,\n",
    "    )\n",
    "    .pipe(combine_emission_pdf)\n",
    "    .rename_vars({\"pdf\": \"emission\"})\n",
    "    .drop_vars([\"final\", \"initial\"])\n",
    ")\n",
    "states = xr.open_dataset(\n",
    "    f\"{target_root}/states.zarr\", engine=\"zarr\", chunks={}, inline_array=True\n",
    ").where(emission[\"mask\"])\n",
    "data = xr.merge([states, emission.drop_vars([\"mask\"])])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dd6816-0d3b-4043-89fa-88a713d02311",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1 = visualization.plot_map(data[\"states\"])\n",
    "plot2 = visualization.plot_map(data[\"emission\"])\n",
    "\n",
    "hv.Layout([plot1, plot2]).cols(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947347d8-9b8c-492f-a820-cb3b6dbc69c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mov = xmovie.Movie(\n",
    "    data.pipe(lambda ds: ds.merge(ds[[\"longitude\", \"latitude\"]].compute())).pipe(\n",
    "        visualization.filter_by_states\n",
    "    ),\n",
    "    plotfunc=visualization.create_frame,\n",
    "    input_check=False,\n",
    "    pixelwidth=15 * 400,\n",
    "    pixelheight=12 * 400,\n",
    "    dpi=400,\n",
    ")\n",
    "\n",
    "mov.save(f\"{target_root}/states.mp4\", parallel=True, overwrite_existing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b1a1bf-741e-4d06-a10d-0622ec6b2d79",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
