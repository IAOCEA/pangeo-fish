{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# **This notebook aims to extract data from a correctly formatted CSV file and adapt it to the pangeo-fish format**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### **Necessary imports**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from data_conversion import extract_tagging_events\n",
    "from data_conversion import show_data_csv\n",
    "from data_conversion import create_metadata_file\n",
    "from data_conversion import extract_name\n",
    "from data_conversion import format_date\n",
    "from data_conversion import extract_DST\n",
    "from data_conversion import convert_to_utc_with_formatting\n",
    "from data_conversion import compat_checking\n",
    "import pytz\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test with the tag NO_A12667\n",
    "### These two paths will be used as an example to see if the full data extraction works correctly\n",
    "\n",
    "csv_path = \"../../all_raw/NO_A12667.CSV\"  # Path to the raw csv file, where the code will extract data from. Update with yours to adapt\n",
    "destination = \"../../all_cleaned/NO_A12267/\"  # Folder where you want to write your the different files. Update with yours to adapt\n",
    "os.makedirs(destination, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "___\n",
    "### 1. **Extracting the tagging events**\n",
    "In this section, we try to test and compare how to extract the necessary information for the tagging events (i.e., time and position for release, fish death, (recapture?))\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "See below the steps that the extract_DST function does: \n",
    "\n",
    "- **Purpose**:\n",
    "  - Extracts releasing date, presumed fish death date, and fish release/recapture positions from a CSV file.\n",
    "\n",
    "- **Initialization**:\n",
    "  - Initializes variables for storing dates (`release_date`, `fish_death`) and coordinates (`lon`, `lat`).\n",
    "\n",
    "- **Processing CSV**:\n",
    "  - Opens the CSV file and iterates through each line.\n",
    "\n",
    "- **Data Extraction**:\n",
    "  - Extracts releasing date and presumed fish death date.\n",
    "  - Formats latitude and longitude coordinates for fish release/recapture positions.\n",
    "\n",
    "- **DataFrame Creation**:\n",
    "  - Constructs a DataFrame with event names, dates, longitude, and latitude.\n",
    "  - Returns the DataFrame containing tagging events data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### See the function tagging_events in the file data_conversion.py for further information\n",
    "tagging_events = extract_tagging_events(csv_path)\n",
    "te_save_path = destination + \"tagging_events.csv\"\n",
    "tagging_events.to_csv(te_save_path, index=False)\n",
    "tagging_events"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7",
   "metadata": {},
   "source": [
    "### Comparing with an actual tagging_events.csv file to check the format\n",
    "cleaned_tagging_events = pd.read_csv(\"cleaned_acoustic/A19124/tagging_events.csv\")\n",
    "cleaned_tagging_events"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8",
   "metadata": {},
   "source": [
    "test_path = destination + \"tagging_events.csv\"\n",
    "check_path = \"cleaned_acoustic/A19124/tagging_events.csv\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "9",
   "metadata": {},
   "source": [
    "### Check if the data type and the columns names and order is matching\n",
    "compat_checking(test_path,check_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "___\n",
    "### 2. **Creating the metadata JSON file**\n",
    "In this section, we try to test and compare how to extract the necessary information for the metatdat.json file.  \n",
    "___\n",
    "- **Purpose**:\n",
    "  - Creates a metadata JSON file based on provided data path and destination path.\n",
    "\n",
    "- **Initialization**:\n",
    "  - Retrieves tag name from the provided file path using a helper function.\n",
    "  - Initializes metadata with tag ID, scientific name, common name, and project information.\n",
    "\n",
    "- **Metadata Construction**:\n",
    "  - Constructs a dictionary (`metadata`) containing tag ID, scientific name (\"Dicentrarchus labrax\"), common name (\"European seabass\"), and project name (\"BARGIP\").\n",
    "\n",
    "- **File Writing**:\n",
    "  - Specifies the filename as \"metadata.json\" and constructs the full destination path.\n",
    "  - Writes the metadata dictionary to a JSON file at the destination path.\n",
    "\n",
    "- **Result**:\n",
    "  - No return value; a metadata JSON file is created at the specified destination path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "### See data_conversion.py for more information about create_metadata_file function\n",
    "create_metadata_file(csv_path, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "___\n",
    "### 3. **Creating the dst.csv file**\n",
    "In this section, we will create the dst file that contains the pressure, temperature and time data.  \n",
    "See below the steps that the extract_DST function does:  \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "- **Opening the CSV File**:\n",
    "  - Takes a file path to a CSV file containing time series data.\n",
    "  - Opens the CSV file using the `csv.reader` object.\n",
    "  \n",
    "- **Iterating Through CSV Rows**:\n",
    "  - Iterates through each row of the CSV file.\n",
    "  \n",
    "- **Extracting Tag ID**:\n",
    "  - Extracts the tag ID from the file path using the `extract_name` function (not provided).\n",
    "  \n",
    "- **Finding Target Line**:\n",
    "  - Searches for the line that contains the headers for the data of interest (\"Date/Time Stamp\", \"Pressure\", \"Temp\").\n",
    "  \n",
    "- **Reading Data**:\n",
    "  - Once the target line is found, starts reading data rows.\n",
    "  \n",
    "- **Formatting Date and Time**:\n",
    "  - Formats the date and time column using the `convert_to_utc_with_formatting` function.\n",
    "  - Converts the local time to UTC time based on the specified time zone.\n",
    "  \n",
    "- **Converting Data Types**:\n",
    "  - Converts the pressure and temperature data from strings to `numpy.float64` for numerical analysis.\n",
    "  \n",
    "- **Storing Data**:\n",
    "  - Stores the formatted data into a list for further processing.\n",
    "  \n",
    "- **Creating DataFrame**:\n",
    "  - After reading all data rows, converts the list of data into a Pandas DataFrame with columns ['time', 'pressure', 'temperature'].\n",
    "  \n",
    "- **Completeness Check**:\n",
    "  - Checks if the number of data points extracted matches the expected length.\n",
    "  - If they match, indicates completion; otherwise, suggests potential incompleteness.\n",
    "  \n",
    "- **Returning DataFrame**:\n",
    "  - Finally, returns the DataFrame containing the extracted data.\n",
    "\n",
    "This function primarily focuses on extracting time, pressure, and temperature data from a CSV file, converting the date and time to UTC time, and formatting the data for analysis. Check the function in the file data_conversion.py for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_zone = \"Europe/Paris\"\n",
    "dst = extract_DST(csv_path, time_zone)\n",
    "dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_save_path = destination + \"dst.csv\"\n",
    "dst.to_csv(dst_save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "___\n",
    "### 4. **Convert and format everything under the raw_test folder to the cleaned folder**\n",
    "This section has test purpose to see if it's easy and works correctly for the different tags in the **raw_test** folder.  \n",
    "Afterwards, the purpose is to do the same operation on the **raw** folder\n",
    "___\n",
    "#### Explenation of the code below :\n",
    "- **Folders and Time Zone Setup**:\n",
    "  - Defines folders (`raw_folder`, `destination_folder`) and time zone (`time_zone`).\n",
    "\n",
    "- **Destination Folder Creation**:\n",
    "  - Checks if the destination folder exists; if not, creates it.\n",
    "\n",
    "- **Processing Raw Data**:\n",
    "  - Iterates through raw files in the raw folder.\n",
    "  - Extracts tag ID and constructs destination paths.\n",
    "  - Creates tag-specific folders if they don't exist.\n",
    "  - Extracts tagging events and DST data from raw files.\n",
    "  - Saves extracted data to CSV files in respective tag folders.\n",
    "  - Creates metadata files for each raw file.\n",
    "\n",
    "- **Handling Incorrect Raw Folder**:\n",
    "  - Prints a message if the raw folder doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "raw_folder = \"all_raw/\"  # Folder name to explore\n",
    "destination_folder = \"all_cleaned/\"\n",
    "time_zone = \"Europe/Paris\"\n",
    "\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.mkdir(destination_folder)\n",
    "\n",
    "# Check if the folder exists\n",
    "if os.path.exists(raw_folder):\n",
    "\n",
    "    # Get list of files to iterate through\n",
    "    files = [\n",
    "        f for f in os.listdir(raw_folder) if os.path.isfile(os.path.join(raw_folder, f))\n",
    "    ]\n",
    "\n",
    "    # Wrap files list with tqdm for progress bar\n",
    "    for file_name in tqdm(files, desc=\"Processing files\"):\n",
    "        raw_file = os.path.join(raw_folder, file_name)\n",
    "\n",
    "        # Extract filename without extension\n",
    "        tag_id = extract_name(raw_file)\n",
    "        destination_path = os.path.join(destination_folder, tag_id)\n",
    "\n",
    "        # Check if the folder for the tag exists, if not, create it\n",
    "        if not os.path.exists(destination_path):\n",
    "            print(\"Creating folder for tag:\", tag_id)\n",
    "            os.mkdir(destination_path)\n",
    "\n",
    "        ### Extracting tagging events from raw file\n",
    "        tag_events = extract_tagging_events(raw_file)\n",
    "        tagging_events_path = os.path.join(destination_path, \"tagging_events.csv\")\n",
    "        tag_events.to_csv(\n",
    "            tagging_events_path, index=False\n",
    "        )  ### Saving them at the right path\n",
    "\n",
    "        ### Extracting DST from raw file\n",
    "        tag_dst = extract_DST(raw_file, time_zone)\n",
    "        dst_path = os.path.join(destination_path, \"dst.csv\")\n",
    "        tag_dst.to_csv(dst_path, index=False)  ### Saving them at the right path\n",
    "\n",
    "        ###Creating metadata files\n",
    "        print(\"creating_metadata\")\n",
    "        create_metadata_file(raw_file, destination_path)\n",
    "\n",
    "else:\n",
    "    print(\"Wrong folder for raw files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### 5. **Warm plume detections**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_name = \"DK_A10531\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f\"../../all_cleaned/{tag_name}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_path = f\"../../all_cleaned/{tag_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wp_timestamps(data_path, storage_path):\n",
    "    # read tag data\n",
    "    df = pd.read_csv(f\"{data_path}dst.csv\", delimiter=\",\")\n",
    "\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "\n",
    "    df = df.set_index(\"time\")\n",
    "\n",
    "    # format\n",
    "    df_d = df.loc[\n",
    "        df.groupby(pd.Grouper(freq=\"D\"))[\"temperature\"].idxmax()\n",
    "    ]  # nan introduced because of daylight saving time change\n",
    "    # df_d = df.groupby(pd.Grouper(freq='D')).mean() # nan introduced because of daylight saving time change\n",
    "    idx = np.where(np.isnan(df_d[\"temperature\"]))[0]\n",
    "    for i in idx:\n",
    "        df_d[\"pressure\"][i] = df_d[\"pressure\"][\n",
    "            i - 1\n",
    "        ]  # put the pressure value of the hour before\n",
    "        df_d[\"temperature\"][i] = df_d[\"temperature\"][\n",
    "            i - 1\n",
    "        ]  # put the temperature value of the hour before\n",
    "    df_d[\"dTemp\"] = np.append(np.diff(df_d[\"temperature\"]), 0.0)\n",
    "    df_d[\"dPressure\"] = np.append(np.diff(df_d[\"pressure\"]), 0.0)\n",
    "    df_d.index = pd.DatetimeIndex(df_d.index).to_period(\"D\")\n",
    "\n",
    "    clf_model = sm.tsa.MarkovAutoregression(\n",
    "        df_d[\"temperature\"], k_regimes=2, order=1, switching_ar=False\n",
    "    )\n",
    "    res_clf_model = clf_model.fit(method=\"bfgs\")\n",
    "    res_clf_model.summary()\n",
    "\n",
    "    predicted_label = res_clf_model.smoothed_marginal_probabilities[0] > 0.50\n",
    "    df_d[\"predicted_label\"] = np.append(0.0, predicted_label)\n",
    "    if (\n",
    "        df_d[\"temperature\"][df_d[\"predicted_label\"] == 1.0].mean()\n",
    "        < df_d[\"temperature\"][df_d[\"predicted_label\"] == 0.0].mean()\n",
    "    ):\n",
    "        df_d[\"predicted_label\"] = df_d[\"predicted_label\"] + 1.0\n",
    "        df_d[\"predicted_label\"][df_d[\"predicted_label\"] == 2.0] = 0.0\n",
    "\n",
    "    if not os.path.exists(storage_path):\n",
    "        os.makedirs(storage_path)\n",
    "\n",
    "    df_d[\"predicted_label\"].to_csv(f\"{storage_path}/detection.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "___\n",
    "### Running for multiple tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = pd.read_csv(\"bar_flag_warm_plume.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_tag = list(detections[detections[\"warm_plume\"] == True][\"tag_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag_name in tqdm(detections_tag, desc=\"\"):\n",
    "    data_path = f\"../../all_cleaned/{tag_name}/\"\n",
    "    storage_path = f\"../../all_cleaned/{tag_name}\"\n",
    "    get_wp_timestamps(data_path, storage_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you want to put all tags in it\n",
    "\n",
    "import s3fs\n",
    "\n",
    "s3 = s3fs.S3FileSystem(\n",
    "    anon=False,\n",
    "    client_kwargs={\n",
    "        \"endpoint_url\": \"https://s3.gra.perf.cloud.ovh.net\",\n",
    "    },\n",
    ")\n",
    "s3.put(\"../../all_cleaned/\", \"gfts-ifremer/tags/bargip/cleaned\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you want only to update the files with detections\n",
    "\n",
    "import s3fs\n",
    "\n",
    "s3 = s3fs.S3FileSystem(\n",
    "    anon=False,\n",
    "    client_kwargs={\n",
    "        \"endpoint_url\": \"https://s3.gra.perf.cloud.ovh.net\",\n",
    "    },\n",
    ")\n",
    "\n",
    "for tag in detections_tag:\n",
    "    s3.put(\n",
    "        f\"../../all_cleaned/{tag}\",\n",
    "        f\"gfts-ifremer/tags/bargip/cleaned/{tag}\",\n",
    "        recursive=True,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
