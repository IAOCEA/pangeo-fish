{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b2162c-86a6-4830-a706-32a2b3052564",
   "metadata": {},
   "source": [
    "# **This notebook aims to extract data from a correctly formatted CSV file and adapt it to the pangeo-fish format**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00244c-fe99-4c46-b651-aab892497506",
   "metadata": {},
   "source": [
    "### **Necessary imports**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf0e3fa4-a1ec-4df6-93e8-d0318869c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from data_conversion import extract_tagging_events\n",
    "from data_conversion import show_data_csv\n",
    "from data_conversion import create_metadata_file\n",
    "from data_conversion import extract_name\n",
    "from data_conversion import format_date\n",
    "from data_conversion import extract_DST\n",
    "from data_conversion import convert_to_utc_with_formatting\n",
    "from data_conversion import compat_checking\n",
    "import pytz\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30ec15c7-0045-4124-b40e-4e294ed7b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test with the tag NO_A12667\n",
    "### These two paths will be used as an example to see if the full data extraction works correctly\n",
    "\n",
    "csv_path = \"../../all_raw/NO_A12667.CSV\" # Path to the raw csv file, where the code will extract data from. Update with yours to adapt \n",
    "destination = \"../../all_cleaned/NO_A12267/\" # Folder where you want to write your the different files. Update with yours to adapt \n",
    "os.makedirs(destination,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9febac8-6c0e-433f-a637-4eb1dd3569ac",
   "metadata": {},
   "source": [
    "___\n",
    "### 1. **Extracting the tagging events**\n",
    "In this section, we try to test and compare how to extract the necessary information for the tagging events (i.e., time and position for release, fish death, (recapture?))\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5549ae-3722-40a3-8dc6-4b86c7958d02",
   "metadata": {},
   "source": [
    "See below the steps that the extract_DST function does: \n",
    "\n",
    "- **Purpose**:\n",
    "  - Extracts releasing date, presumed fish death date, and fish release/recapture positions from a CSV file.\n",
    "\n",
    "- **Initialization**:\n",
    "  - Initializes variables for storing dates (`release_date`, `fish_death`) and coordinates (`lon`, `lat`).\n",
    "\n",
    "- **Processing CSV**:\n",
    "  - Opens the CSV file and iterates through each line.\n",
    "\n",
    "- **Data Extraction**:\n",
    "  - Extracts releasing date and presumed fish death date.\n",
    "  - Formats latitude and longitude coordinates for fish release/recapture positions.\n",
    "\n",
    "- **DataFrame Creation**:\n",
    "  - Constructs a DataFrame with event names, dates, longitude, and latitude.\n",
    "  - Returns the DataFrame containing tagging events data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d497b3ca-87cb-4269-9ea2-a57d775404a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_name</th>\n",
       "      <th>time</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>release</td>\n",
       "      <td>2016-09-01T14:32:00Z</td>\n",
       "      <td>-2.275</td>\n",
       "      <td>47.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fish_death</td>\n",
       "      <td>2017-11-24T16:48:00Z</td>\n",
       "      <td>-2.257</td>\n",
       "      <td>47.101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_name                  time  longitude  latitude\n",
       "0     release  2016-09-01T14:32:00Z     -2.275    47.108\n",
       "1  fish_death  2017-11-24T16:48:00Z     -2.257    47.101"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### See the function tagging_events in the file data_conversion.py for further information\n",
    "tagging_events = extract_tagging_events(csv_path)\n",
    "te_save_path = destination + \"tagging_events.csv\"\n",
    "tagging_events.to_csv(te_save_path,index=False)\n",
    "tagging_events"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8853ab2d-9551-44a4-8077-7b7d69cf10a9",
   "metadata": {},
   "source": [
    "### Comparing with an actual tagging_events.csv file to check the format\n",
    "cleaned_tagging_events = pd.read_csv(\"cleaned_acoustic/A19124/tagging_events.csv\")\n",
    "cleaned_tagging_events"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96094376-b820-48b3-b2b0-f6076b3e02dc",
   "metadata": {},
   "source": [
    "test_path = destination + \"tagging_events.csv\"\n",
    "check_path = \"cleaned_acoustic/A19124/tagging_events.csv\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "0eb2b177-e8e2-48f3-96f0-359bf0a4d7ef",
   "metadata": {},
   "source": [
    "### Check if the data type and the columns names and order is matching\n",
    "compat_checking(test_path,check_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd639af-9ae5-4acf-a6cc-80c98c864f7a",
   "metadata": {},
   "source": [
    "___\n",
    "### 2. **Creating the metadata JSON file**\n",
    "In this section, we try to test and compare how to extract the necessary information for the metatdat.json file.  \n",
    "___\n",
    "- **Purpose**:\n",
    "  - Creates a metadata JSON file based on provided data path and destination path.\n",
    "\n",
    "- **Initialization**:\n",
    "  - Retrieves tag name from the provided file path using a helper function.\n",
    "  - Initializes metadata with tag ID, scientific name, common name, and project information.\n",
    "\n",
    "- **Metadata Construction**:\n",
    "  - Constructs a dictionary (`metadata`) containing tag ID, scientific name (\"Dicentrarchus labrax\"), common name (\"European seabass\"), and project name (\"BARGIP\").\n",
    "\n",
    "- **File Writing**:\n",
    "  - Specifies the filename as \"metadata.json\" and constructs the full destination path.\n",
    "  - Writes the metadata dictionary to a JSON file at the destination path.\n",
    "\n",
    "- **Result**:\n",
    "  - No return value; a metadata JSON file is created at the specified destination path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1a9ba62-2d65-4d3d-bc26-9d52a0cc85ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### See data_conversion.py for more information about create_metadata_file function\n",
    "create_metadata_file(csv_path,destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9855de-f094-4db2-9131-88430115ad44",
   "metadata": {},
   "source": [
    "___\n",
    "### 3. **Creating the dst.csv file**\n",
    "In this section, we will create the dst file that contains the pressure, temperature and time data.  \n",
    "See below the steps that the extract_DST function does:  \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7096f963-a919-4dd7-97d1-34eff6ebbf3f",
   "metadata": {},
   "source": [
    "- **Opening the CSV File**:\n",
    "  - Takes a file path to a CSV file containing time series data.\n",
    "  - Opens the CSV file using the `csv.reader` object.\n",
    "  \n",
    "- **Iterating Through CSV Rows**:\n",
    "  - Iterates through each row of the CSV file.\n",
    "  \n",
    "- **Extracting Tag ID**:\n",
    "  - Extracts the tag ID from the file path using the `extract_name` function (not provided).\n",
    "  \n",
    "- **Finding Target Line**:\n",
    "  - Searches for the line that contains the headers for the data of interest (\"Date/Time Stamp\", \"Pressure\", \"Temp\").\n",
    "  \n",
    "- **Reading Data**:\n",
    "  - Once the target line is found, starts reading data rows.\n",
    "  \n",
    "- **Formatting Date and Time**:\n",
    "  - Formats the date and time column using the `convert_to_utc_with_formatting` function.\n",
    "  - Converts the local time to UTC time based on the specified time zone.\n",
    "  \n",
    "- **Converting Data Types**:\n",
    "  - Converts the pressure and temperature data from strings to `numpy.float64` for numerical analysis.\n",
    "  \n",
    "- **Storing Data**:\n",
    "  - Stores the formatted data into a list for further processing.\n",
    "  \n",
    "- **Creating DataFrame**:\n",
    "  - After reading all data rows, converts the list of data into a Pandas DataFrame with columns ['time', 'pressure', 'temperature'].\n",
    "  \n",
    "- **Completeness Check**:\n",
    "  - Checks if the number of data points extracted matches the expected length.\n",
    "  - If they match, indicates completion; otherwise, suggests potential incompleteness.\n",
    "  \n",
    "- **Returning DataFrame**:\n",
    "  - Finally, returns the DataFrame containing the extracted data.\n",
    "\n",
    "This function primarily focuses on extracting time, pressure, and temperature data from a CSV file, converting the date and time to UTC time, and formatting the data for analysis. Check the function in the file data_conversion.py for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0c77590-3f0b-469a-89b4-a08252759068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction for tag NO_A12667 complete, no missing data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>temperature</th>\n",
       "      <th>pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-08-31T22:00:00Z</td>\n",
       "      <td>23.594</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-08-31T22:01:30Z</td>\n",
       "      <td>23.594</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-08-31T22:03:00Z</td>\n",
       "      <td>23.563</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-08-31T22:04:30Z</td>\n",
       "      <td>23.563</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-08-31T22:06:00Z</td>\n",
       "      <td>23.563</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482647</th>\n",
       "      <td>2018-01-16T17:10:30Z</td>\n",
       "      <td>20.000</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482648</th>\n",
       "      <td>2018-01-16T17:12:00Z</td>\n",
       "      <td>20.203</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482649</th>\n",
       "      <td>2018-01-16T17:13:30Z</td>\n",
       "      <td>20.328</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482650</th>\n",
       "      <td>2018-01-16T17:15:00Z</td>\n",
       "      <td>20.438</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482651</th>\n",
       "      <td>2018-01-16T17:16:30Z</td>\n",
       "      <td>20.547</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>482652 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        time  temperature  pressure\n",
       "0       2016-08-31T22:00:00Z       23.594      1.75\n",
       "1       2016-08-31T22:01:30Z       23.594      1.75\n",
       "2       2016-08-31T22:03:00Z       23.563      1.68\n",
       "3       2016-08-31T22:04:30Z       23.563      1.68\n",
       "4       2016-08-31T22:06:00Z       23.563      1.75\n",
       "...                      ...          ...       ...\n",
       "482647  2018-01-16T17:10:30Z       20.000      0.37\n",
       "482648  2018-01-16T17:12:00Z       20.203      0.37\n",
       "482649  2018-01-16T17:13:30Z       20.328      0.43\n",
       "482650  2018-01-16T17:15:00Z       20.438      0.43\n",
       "482651  2018-01-16T17:16:30Z       20.547      0.18\n",
       "\n",
       "[482652 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_zone = \"Europe/Paris\"\n",
    "dst = extract_DST(csv_path,time_zone)\n",
    "dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90417035-f1c9-4d72-80d3-384404c618be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_save_path = destination + \"dst.csv\"\n",
    "dst.to_csv(dst_save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdf61e0d-4a6e-40ae-8c38-6cd016fa5c60",
   "metadata": {},
   "source": [
    "___\n",
    "### 4. **Convert and format everything under the raw_test folder to the cleaned folder**\n",
    "This section has test purpose to see if it's easy and works correctly for the different tags in the **raw_test** folder.  \n",
    "Afterwards, the purpose is to do the same operation on the **raw** folder\n",
    "___\n",
    "#### Explenation of the code below :\n",
    "- **Folders and Time Zone Setup**:\n",
    "  - Defines folders (`raw_folder`, `destination_folder`) and time zone (`time_zone`).\n",
    "\n",
    "- **Destination Folder Creation**:\n",
    "  - Checks if the destination folder exists; if not, creates it.\n",
    "\n",
    "- **Processing Raw Data**:\n",
    "  - Iterates through raw files in the raw folder.\n",
    "  - Extracts tag ID and constructs destination paths.\n",
    "  - Creates tag-specific folders if they don't exist.\n",
    "  - Extracts tagging events and DST data from raw files.\n",
    "  - Saves extracted data to CSV files in respective tag folders.\n",
    "  - Creates metadata files for each raw file.\n",
    "\n",
    "- **Handling Incorrect Raw Folder**:\n",
    "  - Prints a message if the raw folder doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b345ec-d9f9-494c-a4c7-fc0fce417f87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "raw_folder = \"all_raw/\"  # Folder name to explore\n",
    "destination_folder = \"all_cleaned/\"\n",
    "time_zone = \"Europe/Paris\"\n",
    "\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.mkdir(destination_folder)\n",
    "\n",
    "# Check if the folder exists\n",
    "if os.path.exists(raw_folder):\n",
    "    \n",
    "    # Get list of files to iterate through\n",
    "    files = [f for f in os.listdir(raw_folder) if os.path.isfile(os.path.join(raw_folder, f))]\n",
    "    \n",
    "    # Wrap files list with tqdm for progress bar\n",
    "    for file_name in tqdm(files, desc=\"Processing files\"):\n",
    "        raw_file = os.path.join(raw_folder, file_name)\n",
    "        \n",
    "        # Extract filename without extension\n",
    "        tag_id = extract_name(raw_file)\n",
    "        destination_path = os.path.join(destination_folder, tag_id)\n",
    "        \n",
    "        # Check if the folder for the tag exists, if not, create it\n",
    "        if not os.path.exists(destination_path):\n",
    "            print(\"Creating folder for tag:\", tag_id)\n",
    "            os.mkdir(destination_path)\n",
    "            \n",
    "        ### Extracting tagging events from raw file\n",
    "        tag_events = extract_tagging_events(raw_file)\n",
    "        tagging_events_path =os.path.join(destination_path, \"tagging_events.csv\")\n",
    "        tag_events.to_csv(tagging_events_path,index=False) ### Saving them at the right path\n",
    "\n",
    "        ### Extracting DST from raw file\n",
    "        tag_dst = extract_DST(raw_file,time_zone)\n",
    "        dst_path = os.path.join(destination_path, \"dst.csv\")\n",
    "        tag_dst.to_csv(dst_path, index=False) ### Saving them at the right path\n",
    "        \n",
    "        ###Creating metadata files\n",
    "        print(\"creating_metadata\")\n",
    "        create_metadata_file(raw_file, destination_path)\n",
    "\n",
    "else:\n",
    "    print(\"Wrong folder for raw files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f946d9fa-be9e-42a8-a73f-e0d0f1ba52c9",
   "metadata": {},
   "source": [
    "### 5. **Warm plume detections**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6df38d9-800a-4ab2-9f8e-c4da61af8e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_name = \"DK_A10531\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3fea95f-2651-4cb4-9378-d10dfd9551e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f\"../../all_cleaned/{tag_name}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2d4d6ce7-0ec0-415f-bc3f-e99085022530",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_path = f\"../../all_cleaned/{tag_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b1db843-bead-4b0e-883b-66fc18bd33ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wp_timestamps(data_path,storage_path):\n",
    "    # read tag data\n",
    "    df = pd.read_csv(f\"{data_path}dst.csv\", delimiter = ',')\n",
    "    \n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    df = df.set_index('time')\n",
    "    \n",
    "    # format\n",
    "    df_d = df.loc[df.groupby(pd.Grouper(freq='D'))['temperature'].idxmax()] # nan introduced because of daylight saving time change\n",
    "    #df_d = df.groupby(pd.Grouper(freq='D')).mean() # nan introduced because of daylight saving time change\n",
    "    idx = np.where(np.isnan(df_d['temperature']))[0]\n",
    "    for i in idx:\n",
    "        df_d['pressure'][i] = df_d['pressure'][i-1] # put the pressure value of the hour before\n",
    "        df_d['temperature'][i] = df_d['temperature'][i-1] # put the temperature value of the hour before\n",
    "    df_d['dTemp'] = np.append(np.diff(df_d['temperature']), 0.0)\n",
    "    df_d['dPressure'] = np.append(np.diff(df_d['pressure']), 0.0)\n",
    "    df_d.index = pd.DatetimeIndex(df_d.index).to_period('D')\n",
    "    \n",
    "    clf_model = sm.tsa.MarkovAutoregression(df_d['temperature'], k_regimes=2, order=1, switching_ar=False)\n",
    "    res_clf_model = clf_model.fit(method='bfgs')\n",
    "    res_clf_model.summary()\n",
    "    \n",
    "    predicted_label = res_clf_model.smoothed_marginal_probabilities[0] > 0.50\n",
    "    df_d['predicted_label'] = np.append(0.0,predicted_label)\n",
    "    if df_d['temperature'][df_d['predicted_label']==1.0].mean() < df_d['temperature'][df_d['predicted_label']==0.0].mean():\n",
    "        df_d['predicted_label'] = df_d['predicted_label'] + 1.0\n",
    "        df_d['predicted_label'][df_d['predicted_label'] == 2.0] = 0.0\n",
    "\n",
    "    if not os.path.exists(storage_path):\n",
    "        os.makedirs(storage_path)\n",
    "\n",
    "    df_d[\"predicted_label\"].to_csv(f\"{storage_path}/detection.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c76c19-1e6c-4474-81b4-b3b8cad03b98",
   "metadata": {},
   "source": [
    "___\n",
    "### Running for multiple tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "73ef295f-b617-4c9c-ab78-316b71ff5618",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = pd.read_csv(\"bar_flag_warm_plume.txt\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "225be4a8-a38b-494f-a285-1f58adb883c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_tag = list(detections[detections[\"warm_plume\"] == True][\"tag_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "91c9a09c-597e-4d64-8002-6e3ea583a428",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_2732/892322880.py:18: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df_d.index = pd.DatetimeIndex(df_d.index).to_period('D')\n",
      "/tmp/ipykernel_2732/892322880.py:28: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_d['predicted_label'][df_d['predicted_label'] == 2.0] = 0.0\n",
      "  6%|▌         | 1/18 [00:00<00:09,  1.71it/s]/tmp/ipykernel_2732/892322880.py:18: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df_d.index = pd.DatetimeIndex(df_d.index).to_period('D')\n",
      " 11%|█         | 2/18 [00:00<00:07,  2.15it/s]/tmp/ipykernel_2732/892322880.py:18: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df_d.index = pd.DatetimeIndex(df_d.index).to_period('D')\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      " 17%|█▋        | 3/18 [00:02<00:16,  1.10s/it]/tmp/ipykernel_2732/892322880.py:18: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df_d.index = pd.DatetimeIndex(df_d.index).to_period('D')\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/tmp/ipykernel_2732/892322880.py:28: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_d['predicted_label'][df_d['predicted_label'] == 2.0] = 0.0\n",
      " 22%|██▏       | 4/18 [00:03<00:12,  1.14it/s]/tmp/ipykernel_2732/892322880.py:18: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df_d.index = pd.DatetimeIndex(df_d.index).to_period('D')\n",
      " 28%|██▊       | 5/18 [00:03<00:08,  1.51it/s]/tmp/ipykernel_2732/892322880.py:18: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df_d.index = pd.DatetimeIndex(df_d.index).to_period('D')\n",
      "/tmp/ipykernel_2732/892322880.py:28: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_d['predicted_label'][df_d['predicted_label'] == 2.0] = 0.0\n",
      " 33%|███▎      | 6/18 [00:03<00:06,  1.85it/s]/tmp/ipykernel_2732/892322880.py:18: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df_d.index = pd.DatetimeIndex(df_d.index).to_period('D')\n",
      "/tmp/ipykernel_2732/892322880.py:28: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_d['predicted_label'][df_d['predicted_label'] == 2.0] = 0.0\n",
      " 39%|███▉      | 7/18 [00:04<00:06,  1.71it/s]/tmp/ipykernel_2732/892322880.py:18: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df_d.index = pd.DatetimeIndex(df_d.index).to_period('D')\n",
      "/tmp/ipykernel_2732/892322880.py:28: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_d['predicted_label'][df_d['predicted_label'] == 2.0] = 0.0\n",
      " 44%|████▍     | 8/18 [00:05<00:08,  1.23it/s]/tmp/ipykernel_2732/892322880.py:18: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df_d.index = pd.DatetimeIndex(df_d.index).to_period('D')\n",
      "/tmp/ipykernel_2732/892322880.py:28: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_d['predicted_label'][df_d['predicted_label'] == 2.0] = 0.0\n",
      " 50%|█████     | 9/18 [00:06<00:07,  1.13it/s]/tmp/ipykernel_2732/892322880.py:18: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df_d.index = pd.DatetimeIndex(df_d.index).to_period('D')\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/tmp/ipykernel_2732/892322880.py:28: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_d['predicted_label'][df_d['predicted_label'] == 2.0] = 0.0\n",
      " 56%|█████▌    | 10/18 [00:07<00:05,  1.39it/s]/tmp/ipykernel_2732/892322880.py:18: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df_d.index = pd.DatetimeIndex(df_d.index).to_period('D')\n",
      "/tmp/ipykernel_2732/892322880.py:28: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_d['predicted_label'][df_d['predicted_label'] == 2.0] = 0.0\n",
      " 61%|██████    | 11/18 [00:08<00:05,  1.27it/s]/tmp/ipykernel_2732/892322880.py:18: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df_d.index = pd.DatetimeIndex(df_d.index).to_period('D')\n",
      "/tmp/ipykernel_2732/892322880.py:28: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_d['predicted_label'][df_d['predicted_label'] == 2.0] = 0.0\n",
      " 67%|██████▋   | 12/18 [00:09<00:05,  1.09it/s]/tmp/ipykernel_2732/892322880.py:18: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df_d.index = pd.DatetimeIndex(df_d.index).to_period('D')\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      " 72%|███████▏  | 13/18 [00:10<00:05,  1.04s/it]/tmp/ipykernel_2732/892322880.py:18: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df_d.index = pd.DatetimeIndex(df_d.index).to_period('D')\n",
      " 78%|███████▊  | 14/18 [00:11<00:03,  1.19it/s]/tmp/ipykernel_2732/892322880.py:18: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df_d.index = pd.DatetimeIndex(df_d.index).to_period('D')\n",
      "/tmp/ipykernel_2732/892322880.py:28: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_d['predicted_label'][df_d['predicted_label'] == 2.0] = 0.0\n",
      " 83%|████████▎ | 15/18 [00:11<00:02,  1.32it/s]/tmp/ipykernel_2732/892322880.py:18: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df_d.index = pd.DatetimeIndex(df_d.index).to_period('D')\n",
      "/tmp/ipykernel_2732/892322880.py:28: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_d['predicted_label'][df_d['predicted_label'] == 2.0] = 0.0\n",
      " 89%|████████▉ | 16/18 [00:12<00:01,  1.23it/s]/tmp/ipykernel_2732/892322880.py:18: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df_d.index = pd.DatetimeIndex(df_d.index).to_period('D')\n",
      "/tmp/ipykernel_2732/892322880.py:28: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_d['predicted_label'][df_d['predicted_label'] == 2.0] = 0.0\n",
      " 94%|█████████▍| 17/18 [00:13<00:00,  1.27it/s]/tmp/ipykernel_2732/892322880.py:18: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df_d.index = pd.DatetimeIndex(df_d.index).to_period('D')\n",
      "100%|██████████| 18/18 [00:13<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "for tag_name in tqdm(detections_tag, desc=\"\"):\n",
    "    data_path = f\"../../all_cleaned/{tag_name}/\"\n",
    "    storage_path = f\"../../all_cleaned/{tag_name}\"\n",
    "    get_wp_timestamps(data_path,storage_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b9661ff-39c2-4163-b260-73d8e181480c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell if you want to put all tags in it\n",
    "\n",
    "import s3fs\n",
    "\n",
    "s3 = s3fs.S3FileSystem(\n",
    "    anon=False,\n",
    "    client_kwargs={\n",
    "        \"endpoint_url\": \"https://s3.gra.perf.cloud.ovh.net\",\n",
    "    },\n",
    ")\n",
    "s3.put(\"../../all_cleaned/\",\"gfts-ifremer/tags/bargip/cleaned\",recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d07f1816-6e2c-49ad-9937-da5a3c809e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you want only to update the files with detections\n",
    "\n",
    "import s3fs\n",
    "\n",
    "s3 = s3fs.S3FileSystem(\n",
    "    anon=False,\n",
    "    client_kwargs={\n",
    "        \"endpoint_url\": \"https://s3.gra.perf.cloud.ovh.net\",\n",
    "    },\n",
    ")\n",
    "\n",
    "for tag in detections_tag:\n",
    "    s3.put(f\"../../all_cleaned/{tag}\",f\"gfts-ifremer/tags/bargip/cleaned/{tag}\",recursive=True)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
